{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask Learning for BERT Example\n",
    "In this example RuBERT Base is trained on DaNetQA task of Russian SuperGLUE and sentiment analysis task on \"mokoron\" dataset of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T23:03:02.035505Z",
     "start_time": "2022-01-26T23:03:01.079187Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from task import Task\n",
    "import transformers\n",
    "from rutasks import rsg, mokoron\n",
    "import datasets\n",
    "from data_utils import dict_map\n",
    "from multitask_transformers import MultitaskModel, make_features, MultitaskTrainer, NLPDataCollator, unpack_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks are created using Task(...) with `cls` - model class from `transformers` (Huggingface Transformers module), `config` from `transformers`, `data` dataset with parts from `datasets`(Huggingface Datasets module), `converter_to_features` being a function mapping one batch to defaultish features usually fed into models of `transformers` and an optional `name` for verbose messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T23:03:14.603530Z",
     "start_time": "2022-01-26T23:03:02.045861Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset russian_super_glue (/Users/s1m00n/.cache/huggingface/datasets/russian_super_glue/danetqa/0.0.1/6fcadbfc1d8f0298b2f01ff277093772efe9e1b98f3c0df8ab5f511b3b9e13c9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cddc3e20a76c43128378ed9a8bd355a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map(preprocess_danetqa, danetqa):\n",
      "    map(preprocess_danetqa, danetqa[train])\n",
      "    map(preprocess_danetqa, danetqa[validation])\n",
      "    map(preprocess_danetqa, danetqa[test])\n"
     ]
    }
   ],
   "source": [
    "base_model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name)\n",
    "conv_params = {\"padding\": True, \"truncation\": True, \"max_length\": 512}\n",
    "tasks = {\n",
    "    'danetqa': Task(\n",
    "        cls=transformers.AutoModelForSequenceClassification,\n",
    "        config=transformers.AutoConfig.from_pretrained(base_model_name, num_labels = 2),\n",
    "        data=dict_map(datasets.load_dataset(\"russian_super_glue\", 'danetqa'), rsg.preprocess_danetqa, name=\"danetqa\"),\n",
    "        converter_to_features=rsg.InputLabelConv(tokenizer, **conv_params),\n",
    "        name=f\"Russian SuperGLUE: DaNetQA\"\n",
    "    ),\n",
    "    'mokoron': Task(\n",
    "        cls=transformers.AutoModelForSequenceClassification,\n",
    "        config=transformers.AutoConfig.from_pretrained(base_model_name, num_labels = 2),\n",
    "        data=mokoron.load(),\n",
    "        converter_to_features=rsg.InputLabelConv(tokenizer, **conv_params)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making multitask features and unpack splits for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-01-26T23:03:03.076Z"
    }
   },
   "outputs": [],
   "source": [
    "features = make_features(tasks)\n",
    "train, validation = unpack_splits(features, \"train\", \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp\n",
    "train, validation = unpack_splits(features, \"train\", \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = MultitaskModel.create(base_model_name, tasks)\n",
    "trainer = MultitaskTrainer(\n",
    "    model=model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"test\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=100,\n",
    "        eval_steps=1000,\n",
    "        do_train=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=15,\n",
    "        per_device_eval_batch_size=128, # TODO: this value is not read in MultitaskTrainer (strange bug)\n",
    "        save_steps=5000,\n",
    "    ),\n",
    "    data_collator=NLPDataCollator(),\n",
    "    train_dataset=train\n",
    "    eval_dataset=validation\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
