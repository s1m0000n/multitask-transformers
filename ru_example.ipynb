{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitask Learning for BERT Example\n",
    "In this example RuBERT Base is trained on DaNetQA task of Russian SuperGLUE and sentiment analysis task on \"mokoron\" dataset of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T23:03:02.035505Z",
     "start_time": "2022-01-26T23:03:01.079187Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from rutasks import rsg, mokoron\n",
    "import datasets\n",
    "from data_utils import dict_map\n",
    "from multitask_transformers import MultitaskModel, make_features, MultitaskTrainer, NLPDataCollator, unpack_splits, Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks are created using Task(...) with `cls` - model class from `transformers` (Huggingface Transformers module), `config` from `transformers`, `data` dataset with parts from `datasets`(Huggingface Datasets module), `converter_to_features` being a function mapping one batch to defaultish features usually fed into models of `transformers` and an optional `name` for verbose messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-26T23:03:14.603530Z",
     "start_time": "2022-01-26T23:03:02.045861Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model_name = \"DeepPavlov/rubert-base-cased\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(base_model_name)\n",
    "conv_params = {\"pad_to_max_length\": True, \"max_length\": 512}\n",
    "tasks = {\n",
    "    'danetqa': Task(\n",
    "        cls=transformers.AutoModelForSequenceClassification,\n",
    "        config=transformers.AutoConfig.from_pretrained(base_model_name, num_labels = 2),\n",
    "        data=dict_map(datasets.load_dataset(\"russian_super_glue\", 'danetqa'), rsg.preprocess_danetqa, name=\"danetqa\"),\n",
    "        converter_to_features=rsg.InputLabelConv(tokenizer, **conv_params),\n",
    "        name=f\"Russian SuperGLUE: DaNetQA\"\n",
    "    ),\n",
    "    'mokoron': Task(\n",
    "        cls=transformers.AutoModelForSequenceClassification,\n",
    "        config=transformers.AutoConfig.from_pretrained(base_model_name, num_labels = 2),\n",
    "        data=mokoron.load(),\n",
    "        converter_to_features=rsg.InputLabelConv(tokenizer, input_name='text', **conv_params)\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making multitask features and unpack splits for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = make_features(tasks)\n",
    "train, validation = unpack_splits(features, \"train\", \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = MultitaskModel.create(base_model_name, tasks)\n",
    "trainer = MultitaskTrainer(\n",
    "    model=model,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=\"test\",\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=1e-5,\n",
    "        logging_steps=100,\n",
    "        eval_steps=1000,\n",
    "        do_train=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=15,\n",
    "        per_device_eval_batch_size=128, # TODO: this value is not read in MultitaskTrainer (strange bug)\n",
    "        save_steps=5000,\n",
    "    ),\n",
    "    data_collator=NLPDataCollator(),\n",
    "    train_dataset=train,\n",
    "    eval_dataset=validation\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
